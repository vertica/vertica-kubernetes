# (c) Copyright [2021-2024] Open Text.
# Licensed under the Apache License, Version 2.0 (the "License");
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for verticadb-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -------------------------------------------------------------------------------
# NOTE: Whenever changing default values in here, be sure to update the
# README.md in this directory.  That file lists out the helm chart parameters
# with the default values.
# -------------------------------------------------------------------------------

# To control the name of all of the objects created in the helm chart.
# nameOverride: verticadb-operator

image:
  repo: docker.io
  name: opentext/verticadb-operator:25.3.0-0
  pullPolicy: IfNotPresent

imagePullSecrets: null

controllers:
  # If true, the controllers will run watching for changes in any of the
  # Vertica custom resources. Setting this to false and webhook.enabled to true
  # will cause the manager to only serve webhook requests. This mode allows you
  # to decouple the webhook from the controllers, allowing you to run them in
  # different pods.
  enable: true
  # What is the scope of the controllers? Allowed options are: cluster or namespace.
  # Two things happen when the controllers are scoped at the namespace level: we
  # only setup watches for the namespace the operator is deployed in, and we will only
  # grant Role/RoleBindings to the manager for deployed namespace.
  scope: cluster
  # Use this if you want to provide a custom burst size for event recording in the operator.
  # Increasing this allows the controllers to record more events in a short period.
  burstSize: 100
  # Controls the maximum backoff requeue duration (in milliseconds) for the vdb controller.
  # Default value is 1000 ms (1 second). Increase this value to reduce the requeue rate
  # if you have multiple databases running and want a lower rate limit.
  vdbMaxBackoffDuration: 1000
  # Controls the maximum backoff requeue duration (in milliseconds) for the sandbox controller.
  # Default value is 1000 ms (1 second). Increase this value to reduce the requeue rate
  # if you have multiple sandboxes running and want a lower rate limit.
  sandboxMaxBackoffDuration: 1000

webhook:
  # The webhook requires a TLS certificate to work. This parm defines how the
  # cert is supplied. Valid values are:
  # - internal: The certs are generated internally by the operator prior to
  #      starting the managing controller. The generated cert is self-signed.
  #      When it expires, the operator pod will need to be restarted in order
  #      to generate a new certificate. This is the default.
  # - cert-manager: The certs are generated using the cert-manager operator.
  #      This operator needs to be deployed before deploying the operator. It
  #      can be installed with this command:
  #      kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.yaml
  #
  #      Deployment of this chart will create a self-signed cert. The advantage
  #      of this over 'internal' is that cert-manager will automatically handle
  #      private key rotation when the certificate is about to expire.
  # - secret: The certs are created prior to installation of this chart and are
  #      provided to the operator through a secret. This option gives you the
  #      most flexibility as it is entirely up to you how the cert is created.
  #      This option requires the webhook.tlsSecret option to be set. For
  #      backwards compatibility, if webhook.tlsSecret is set, it is implicit
  #      that this mode is selected.
  certSource: internal
  # Use this parameter if you want to provide your own cert. tlsSecret is a
  # name of a secret in the same namespace the chart is being installed in.
  # The secret must have the keys: tls.key and tls.crt. It can also include the
  # key ca.crt. When that key is included the operator will patch it into the CA
  # bundle in the webhook configuration.
  #
  # For backwards compatibility, if this is set, then 'certSource = secret' is
  # implied.
  tlsSecret: ""
  # If true, the webhook will be enabled and its configuration is setup by the helm chart. 
  # Setting this to false will disable the webhook. The webhook setup needs privileges to add 
  # validatingwebhookconfiguration and mutatingwebhookconfiguration, both are cluster scoped. 
  # If you do not have necessary privileges to add these configurations, then this option 
  # can be used to skip that and still deploy the operator.
  enable: true

cache:
  # If set to true, the operator will use cache to store tls certificates.
  # Setting this to false will disable cache in the operator
  enable: false

logging:
  # level is the minimum logging level. Valid values are: debug, info, warn, and error
  level: info

# Controls the amount of concurrency within the operator to handle the various
# CRs we have.
reconcileConcurrency:
  verticadb: 5
  verticaautoscaler: 1
  eventtrigger: 1
  verticarestorepointsquery: 1
  verticascrutinize: 1
  sandboxconfigmap: 1
  verticareplicator: 3

# The resource requirements for the operator pod.  See this for more info:
# https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
# These defaults must be kept in sync with config/manifests/kustomization.yaml
resources:
  limits:
    cpu: 100m
    memory: 750Mi
  requests:
    cpu: 100m
    memory: 20Mi

# Add specific node selector labels to control where the server pod is scheduled.
# If left blank then no selectors are added.
# See: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
# key: value
nodeSelector: {}

# Add any affinity or anti-affinity to the pod to control where it gets scheduled.
# See: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
# podAffinity:
#   requiredDuringSchedulingIgnoredDuringExecution:
#   - labelSelector:
#       matchExpressions:
#       - key: security
#         operator: In
#         values:
#         - S1
#     topologyKey: topology.kubernetes.io/zone
affinity: {}

# PriorityClassName given to Pods of this StatefulSet
# See: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName: ""

# Taints and tolerations.
# See: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
tolerations: []

# Set this if you want to override the default name of the serviceaccount
# serviceAccountNameOverride: ""

# Annotations to be added to the serviceaccount that we create. This is a key/value map.
serviceAccountAnnotations: {}

prometheus:
  # Controls exposing of the prometheus metrics endpoint.  Valid options are:
  #
  # EnableWithAuth: A new service object will be created that exposes the
  #    metrics endpoint.  Access to the metrics are controlled by rbac rules.
  #    The metrics endpoint will use the https scheme.
  # EnableWithoutAuth: Like EnableWithAuth, this will create a service
  #    object to expose the metrics endpoint.  However, there is no authority
  #    checking when using the endpoint.  Anyone who had network access
  #    endpoint (i.e. any pod in k8s) will be able to read the metrics.  The
  #    metrics endpoint will use the http scheme.
  # EnableWithTLS: Like EnableWithAuth, this will create a service
  #    object to expose the metrics endpoint.  However, there is no authority
  #    checking when using the endpoint.  People with network access to the
  #    endpoint (i.e. any pod in k8s) and the correct certs can read the metrics.
  #    The metrics endpoint will use the https scheme. 
  #    It needs to be used with tlsSecret. If tlsSecret is not set, the behavior
  #    will be similar to EnableWithoutAuth, except that the endpoint will use 
  #    https schema.
  # Disable: Prometheus metrics are not exposed at all.
  expose: Disable

  # If prometheus is exposed with an auth proxy (EnableWithAuth), use this
  # parameter to control what certificates are used for the https endpoint. If
  # this is empty, the operator will use a generated self-signed cert. When
  # provided, the certificates can be used to authenticate with the metrics
  # endpoint.
  tlsSecret: ""

  # This controls the creation of ClusterRole/ClusterRoleBinding to access
  # the metrics endpoint.
  createProxyRBAC: true

# Pod security context configuration
securityContext:
  fsGroup: 65532
  runAsGroup: 65532
  runAsNonRoot: true
  runAsUser: 65532
  seccompProfile:
    type: RuntimeDefault

# Container-specific security context configuration
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL

keda:
  # This controls the creation of ClusterRole rules for KEDA objects.
  createRBACRules: true

## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
##
grafana:
  enabled: false # Set to true if you want to deploy Grafana

  ## Override the deployment namespace
  ##
  namespaceOverride: ""

  adminUser: admin
  adminPassword: admin

  admin:
    ## Name of the secret. Can be templated.
    existingSecret: ""
    userKey: admin-user
    passwordKey: admin-password

  replicas: 1

  rbac:
    ## If true, Grafana PSPs will be created
    ##
    pspEnabled: false

  ## Enable persistence using Persistent Volume Claims for Grafana data storage
  ## This stores Grafana's database (users, dashboards, and preferences), plugins, and other persistent files
  ## so they survive pod restarts or upgrades.
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    type: pvc
    enabled: false
    # storageClassName: default
    ## (Optional) Use this to bind the claim to an existing PersistentVolume (PV) by name.
    volumeName: ""
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection

  ## Grafana's primary configuration
  ## NOTE: values in map will be converted to ini format
  ## ref: http://docs.grafana.org/installation/configuration/
  ##
  grafana.ini:
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning

  ingress:
    ## If true, Grafana Ingress will be created
    ##
    enabled: false

  serviceAccount:
    create: true
    serviceAccount.automountServiceAccountToken: true

  # Flag to mark provisioned data sources for deletion if they are no longer configured.
  # It takes no effect if data sources are already listed in the deleteDatasources section.
  # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-configuration-file
  prune: false

  ## Passed to grafana subchart and used by servicemonitor below
  ##
  service:
    portName: http-web
    ipFamilies: []
    ipFamilyPolicy: ""

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'vertica'
        orgId: 1
        folder: 'Vertica'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/vertica
  dashboards:
    vertica:
      # Example 1: Node Exporter Full Dashboard (ID 1860)
      # This dashboard is typically for Prometheus data source.
      vertica-overview-prometheus:
        gnetId: 19917
        # revision: 1 # Optional: Pin to a specific revision if needed
        datasource: Prometheus
      vertica-queries-prometheus:
        gnetId: 19915
        # revision: 1 # Optional: Pin to a specific revision if needed
        datasource: Prometheus
      vertica-resource-management-prometheus:
        gnetId: 19916
        # revision: 1 # Optional: Pin to a specific revision if needed
        datasource: Prometheus
      vertica-depot-prometheus:
        gnetId: 19914
        # revision: 1 # Optional: Pin to a specific revision if needed
        datasource: Prometheus
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://{{ .Release.Name }}-prometheus-server-prometheus.{{ .Release.Namespace }}.svc.cluster.local:9090
          access: proxy
          isDefault: true
          jsonData:
            tlsSkipVerify: true 
        - name: Loki
          type: loki
          url: http://{{ .Release.Name }}-loki-gateway.{{ .Release.Namespace }}.svc.cluster.local:80
          access: proxy
          isDefault: false

prometheusServer:
  enabled: false # Set to true if you want to deploy Prometheus server
  # === Global Namespace Configuration ===
  # By default, kube-prometheus-stack installs into .Release.Namespace
  # You can override it here if you want it in a different namespace:
  # namespaceOverride: "my-monitoring-ns" # Ensure this namespace exists or create it

  # === Control Component Installation (to achieve "only Prometheus") ===
  # Disable components you don't want.
  # KEEP `prometheusOperator` ENABLED as it's required for Prometheus itself.
  alertmanager:
    enabled: false
  grafana:
    enabled: false # If you have Grafana as a separate sub-chart or elsewhere
  kubeControllerManager:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubelet:
    enabled: false
  kubeApiServer:
    enabled: false
  kubeProxy:
    enabled: false
  kubeStateMetrics:
    enabled: false
  nodeExporter:
    enabled: false
  coreDns:
    enabled: false
  prometheus-node-exporter: # Some older versions might use this instead of nodeExporter
    enabled: false

  prometheus-windows-exporter:
    ## Enable ServiceMonitor and set Kubernetes label to use as a job label
    ##
    prometheus:
      monitor:
        enabled: false

  nameOverride: prometheus-server
  
  # Prometheus Operator - keep minimal
  prometheusOperator:
    enabled: true
    # Minimal resources
    resources:
      limits:
        cpu: 200m
        memory: 750Mi
      requests:
        cpu: 100m
        memory: 100Mi
    # Reduce admission webhook complexity
    admissionWebhooks:
      enabled: false
    serviceMonitor:
      selfMonitor: false # Disable self-monitoring ServiceMonitor
    # Disable TLS for simplicity
    tls:
      enabled: false

  # === Prometheus Specific Configuration ===
  prometheus:
    # Service account for Prometheuses to use. 
    serviceAccount:
      create: false
      name: prometheus-vertica-sa
      annotations: {}
      automountServiceAccountToken: true

    service:
      # You can also change the type if needed, e.g., ClusterIP, NodePort, LoadBalancer
      type: ClusterIP
      # NodePort or LoadBalancer can be used if you need external access to Prometheus
      # for instance, for remote scraping or a UI access
      # nodePort: 30090

    serviceMonitor:
      selfMonitor: false
    
    # Storage for Prometheus (highly recommended for production)
    prometheusSpec:
      replicas: 1
      retention: "7d" # How long to keep data
      # Minimal resource allocation
      resources:
        requests:
          memory: "512Mi"
          cpu: "200m"
        limits:
          memory: "1Gi"
          cpu: "500m"
      
      # Reduce retention and storage
      retention: "7d"
      retentionSize: "2GB"
      storageSpec:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 5Gi # Adjust storage size as needed

      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#webtlsconfig
      web: {}

    # Optionally disable the default `kube-prometheus-stack` scrape configs if you're
    # defining your own ServiceMonitors/PodMonitors for your operator.
    # If you want Prometheus to scrape your K8s cluster, keep some of these enabled.
    # scrapeConfig:
    #   enabled: false # This might disable all default scrape configs

  # === ServiceMonitor for your Operator's Metrics (Example) ===
  # If your operator exposes Prometheus metrics, you can define a ServiceMonitor
  # directly in your main chart's templates or here if you want to conditionally
  # deploy it with Prometheus.
  # This example assumes your operator has a service named `my-operator-service`
  # in the same namespace that exposes metrics on port `metrics-port`.
  # For more advanced ServiceMonitor creation, it's often better to create a
  # separate template in your main chart that consumes your operator's service details.
  # This section is just illustrative of where you might define it.
  #
  # serviceMonitor:
  #   # This would typically be a template in your main chart, but showing how you'd
  #   # enable the Prometheus Operator to find your operator's metrics.
  #   name: my-operator-service-monitor
  #   selector:
  #     matchLabels:
  #       app.kubernetes.io/name: my-operator
  #   endpoints:
  #     - port: metrics-port # The name of the port in your operator's Service
  #       path: /metrics
  # Disable all default scraping rules
  defaultRules:
    create: false
    rules:
      alertmanager: false
      etcd: false
      configReloaders: false
      general: false
      k8s: false
      kubeApiserverAvailability: false
      kubeApiserverBurnrate: false
      kubeApiserverHistogram: false
      kubeApiserverSlos: false
      kubelet: false
      kubeProxy: false
      kubePrometheusGeneral: false
      kubePrometheusNodeRecording: false
      kubernetesApps: false
      kubernetesResources: false
      kubernetesStorage: false
      kubernetesSystem: false
      kubeScheduler: false
      kubeStateMetrics: false
      network: false
      node: false
      nodeExporterAlerting: false
      nodeExporterRecording: false
      prometheus: false
      prometheusOperator: false

loki:
  enabled: false # Set to true if you want to deploy loki

  # == Configuration for running Loki
  loki:
    # The Compactor is responsible for compaction of index files and applying log retention.
    compactor:
      retention_enabled: true  # default is not enabled
      working_directory: /var/loki/compactor # Specify a working directory for the compactor
      compaction_interval: 10m # The compactor runs every 10 minutes
      retention_delete_delay: 2h # Deletion of chunks is delayed by 2 hours after the retention period expires
      delete_request_store: filesystem # Store delete requests locally on disk

    # Sets the global retention period to 30 days. Logs older than 30 days will be marked for deletion by the compactor.
    limits_config:
      retention_period: 720h # 30 days

    # Should authentication be enabled
    auth_enabled: false

    # Set parameters shared across multiple Loki components or modules
    commonConfig:
      # Loki stores multiple copies of logs in the ingester component, based on a configurable 
      # replication factor, generally 3.
      replication_factor: 1  # set to 1 for testing purpose

    # How Loki stores and indexes log data over time
    schemaConfig:
      configs:
        - from: 2024-04-01 # Start date for this schema configuration
          store: tsdb      # Time Series Database. Index store type (e.g., tsdb, boltdb)
          object_store: filesystem # Type of object storage (e.g., s3, gcs, filesystem)
          schema: v13      # Schema version
          index:
            prefix: loki_index_  # Prefix for index entries
            period: 24h          # Period for index rotation

    # Loki receives logs in separate streams, where each stream is uniquely identified by its 
    # tenant ID and its set of labels. As log entries from a stream arrive, they are compressed 
    # as chunks and saved in the chunks store.
    # The index stores each stream’s label set and links them to the individual chunks.
    storage:
      type: filesystem  # not typically recommended for production use
      filesystem:
        chunks_directory: "/var/loki/chunks" # Directory for storing log data chunks

  # use filesystem
  minio:
    enabled: false
  # disable canary to skip health check
  lokiCanary:
    enabled: false
  # disable test or you can not disable canary
  test:
    enabled: false

  # Loki offers Monolithic, Simple Scalable Deployment (SSD), and Microservices (Distributed) 
  # modes for running its components. Monolithic is best for testing and small loads, while 
  # Microservices is for massive scale and high availability by running each component independently. 
  # SSD is the recommended default, providing a balance by grouping components into read, write, 
  # and backend targets for independent scaling and managing moderate to large log volumes. 
  deploymentMode: SingleBinary  # monolithic mode
  singleBinary:
    replicas: 1   # You must specify commonConfig.replication_factor: 1 if you are only using 1 replica, otherwise requests will fail.
  
  # Zero out replica counts of other deployment modes
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

alloy:
  enabled: false

  # Define the number of replicas for the deployment.
  replicaCount: 1
  
  # Use self-created RBAC resources for Alloy deployment
  rbac:
    create: false

  # Use self-created service account for Alloy deployment
  serviceAccount:
    create: false
    name: vdb-op-alloy

  # Grafana Alloy specific configuration
  alloy:
    configMap:
      create: false
      name: vdb-op-alloy
      key: config.alloy
