# Introduction

This guide explains how to set up an environment to develop and test the Vertica operator.

# Software Setup
Developing with this repo requires a working Kubernetes cluster. Additionally, the integration tests require the following software:

- [go](https://golang.org/doc/install) (version 1.16.2)
- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) (version 1.20.1).  If you are using a real Kubernetes cluster, this is already installed.
- [helm](https://helm.sh/docs/intro/install/) (version 3.5.0)
- [kubectx](https://github.com/ahmetb/kubectx/releases/download/v0.9.1/kubectx) (version 0.9.1)
- [kubens](https://github.com/ahmetb/kubectx/releases/download/v0.9.1/kubens) (version 0.9.1)
- [golangci-lint](https://golangci-lint.run/usage/install/) (version 1.41.1)
- [krew](https://github.com/kubernetes-sigs/krew/releases/tag/v0.4.1) (version 0.4.1) $HOME/.krew/bin must be in your path
- [stern](https://github.com/stern/stern) (version 1.15.0)
- [kuttl](https://github.com/kudobuilder/kuttl/) (version 0.9.0)
- [changie](https://github.com/miniscruff/changie) (version 0.5.0)

# Repo Structure

- **docker-vertica/**: has the necessary files to build a Vertica server container. The build process requires that you provide a Vertica version 10.1.1 or higher RPM package. The RPM is not included in this repo.
- **docker-operator/**: has the necessary files to build the container that holds the operator
- **docker-vlogger/**: has the necessary files to build the container that runs the vlogger sidecar. This sidecar will tail the vertica.log to stdout. This is used only for development purposes to aid in testing out the sidecar property in the custom resource (CR).
- **scripts/**: contains scripts for the repository. Some scripts run Makefile targes, while others, such as *upgrade-vertica.sh*, automate manual tasks.
- **api/**: defines the spec of the custom resource definition (CRD)
- **pkg/**: includes all of the packages that for the operator
- **cmd/**: contains source code for each of the executables
- **bin/**: contains the compiled or downloaded binaries that this repository depends on
- **config/**: generated files of all of the manifests that make up the operator. Of particular importance is *config/crd/bases/vertica.com_verticadbs.yaml*, which shows a sample spec for our CRD.
- **tests/**: has the test files for e2e and soak testing
- **changes/**: stores the changelog for past releases and details about the changes for the next release
- **hack/**: includes a boilerplate file of a copyright that is included on the generated files
- **helm-charts/**: contains the Helm charts that this repository builds

# Kind Setup
[Kind](https://kind.sigs.k8s.io/) is a way to set up a multi-node Kubernetes cluster using Docker. It mimics a multi-node setup by starting a separate container for each node.  The machine requirements for running Kind are minimal - it is possible to set this up on your own laptop. This is the intended deployment to run the tests in an automated fashion.

We have a wrapper that you can use to set up Kind and create a cluster that is suitable for testing Vertica. The following command creates a cluster named `cluster1` that has one master node and one worker node. It completes after a few minutes:  

```
$ scripts/kind.sh init cluster1
```

When the command completes, change the context to use the cluster. The cluster has its own kubectl context named `kind-cluster1`:

```
$ kubectx kind-cluster1
```

To test the container, check the status of the nodes:

```
$ kubectl get nodes
```


# Kind Cleanup

After you are done with the cluster, you can delete it with our helper script. Substitute `cluster1` with the name of your cluster:

```
$ scripts/kind.sh term cluster1
```

If you forgot the cluster name, run Kind directly to get all of the installed clusters:

```
$ PATH=$PATH:$HOME/go/bin
$ kind get clusters
cluster1
```

## Add a changelog entry

The changelog file is generated by [Changie](https://github.com/miniscruff/changie).  It separates the changelog generation from commit history, so any PR that has a notable change should add new changie entries.

# Developer Workflow

**IMPORTANT**: You must have a Vertica version 10.1.1 or higher RPM to build the *docker-vertica* container. **This repo DOES NOT include an RPM**. 

Store the RPM in the `docker-vertica/packages` directory with the name `vertica-x86_64.RHEL6.latest.rpm`:

```
$ cp /dir/vertica-x86_64.RHEL6.latest.rpm docker-vertica/packages/
```

## 1. Building and Pushing Vertica Container

We currently use the following containers:
- **docker-vertica/Dockerfile**: The long-running container that runs the vertica daemon.
- **docker-operator/Dockerfile**: The container that runs the operator and webhook
- **docker-vlogger/Dockerfile**: The container that runs the vertica logger. It will tail the output of vertica.log to stdout. This is used for testing purposes. Some e2e tests use this as a sidecar to the Vertica server container.

To run Vertica in Kubernetes, we need to package Vertica inside a container. This container is later referenced in the YAML file when we install the Helm chart.

By default, we create containers that are stored in the local docker daemon. The tag is either `latest` or, if running in a Kind environment, it is `kind`. You can control the container names by setting the   following environment variables prior to running the make target.  

- **OPERATOR_IMG**: Operator image name.
- **VERTICA_IMG**: Vertica image name.
- **VLOGGER_IMG**: Vertica logger sidecar image name.

If necessary, these variables can include the url of the registry. For example, `export OPERATOR_IMG=myrepo:5000/verticadb-operator:latest`.

Vertica provides 2 container sizes: the default, full image, and the minimal image that does not include the 240MB Tensorflow package.

1. Run the `docker-build` make target to build the necessary containers. The following command uses the default image:
   ```
   $ make docker-build
   ``` 

   To build the minimal container, invoke the make target with `MINIMAL_IMG=YES`:

   ```
   $ make docker-build MINIMAL_VERTICA_IMG=YES
   ```
   Due to the size of the vertica image, this step can take in excess of 10 minutes when run on a laptop.

2. Make these containers available to the Kubernetes cluster.  Push them to the Kubernetes cluster with the following make target:

   ```
   $ make docker-push
   ```

   This command honors the same environment variables used when creating the image.

If your image builds fail silently, confirm that there is enough disk space in your Docker repository to store the built images.

## 2. Generating controller files

Vertica uses the [operator-sdk framework](https://sdk.operatorframework.io/) for the operator. It provides tools to generate code so that you do not have to manually write boilerplate code. Depending on what you changed you may need to periodically regenerate files with the following command:

```
$ make generate manifests
```

## 3. Running Linters

We run two different linters:
1. **Helm lint**: Uses the chart verification test that is built into Helm.
2. **Go lint**: Uses a few linters that you can run with golang.

Both of these linters can be run with this command:

```
$ make lint
```

## 4. Running Unit Tests

We have unit tests for both the Helm chart and the Go operator.  

Unit tests for the Helm chart are stored in `helm-charts/verticadb-operator/tests`. They use the [unittest plugin for helm](https://github.com/quintush/helm-unittest). Some samples that you can use to write your own tests can be found at [unittest github page](https://github.com/quintush/helm-unittest/tree/master/test/data/v3/basic). For details about the test format, review the [helm-unittest GitHub repo](https://github.com/quintush/helm-unittest/blob/master/DOCUMENT.md).

Unit tests for the Go operator use the Go testing infrastructure. Some of the tests stand up a mock Kubernetes control plane using envtest, and runs the operator against that. Per Go standards, the test files are included in package directories and end with `_test.go`.

The Helm chart testing and Go lang unit tests can be run like this:

```
$ make run-unit-tests
```

## 5. Running the Operator

There are three ways to run the operator:
1. Locally in your shell.
2. Packaged in a container and deployed it in Kubernetes as a deployment object
3. With the [verticadb-operator Helm chart](https://vertica.github.io/charts/).

### Option 1: Locally

This method runs the operator synchronously in your shell. It is the fastest way get up and running, but it does not mimic the way that the operator runs in a real Kubernetes environment.

Enter the following command:

```
$ make install run ENABLE_WEBHOOKS=false
```

Press **Ctrl+C** to stop the operator.

**NOTE:** When you run the operator locally, you can run only ad-hoc tests, not integration and e2e tests

This disables the webhook from runing too, as running the webhook requires TLS certs to be available.

### Option 2: Kubernetes Deployment

You can have the operator watch the entire cluster or a specific namespace.

To watch a specific namespaces, set the **WATCH_NAMESPACE** environment variable. The operator is deployed in that specific namespace and is triggered only if the Vertica database is deployed in that namespace. If you specify a namespace that does not exist, you get an error and the operator deployment is interrupted.

The following commands specify a namespace and deploy the operator:
```
$ WATCH_NAMESPACE=your_namespace
$ make docker-build deploy
```

This runs the operator as a deployment within Kubernetes. To tear down the deployment, run the `undeploy` make target:
```
$ make undeploy
```

### Option 3: Helm Chart release

This is the most convenient way to run the operator. Before you create a release with the Helm chart, you must generate the manifests that Helm uses to install the operator.

1. Run the `create-helm-charts` make target to generate the helm charts:

   ```
   $ make create-helm-charts
   ```

2. Install the verticadb-operator in a specific namespace with the `helm install` command:
   ```
   $ helm install -n random release_name helm-charts/verticadb-operator
   ```
   The previous command does the following:
   - Creates a release named `release_name`
   - Runs the operator in a namespace called **random**
   - Uses the default `image:tag` defined in **.Values.image.name**. Use `--set image.name=<img:tag>` to specify the image name and tag.

To tear down the release with your operator, run `helm uninstall` with the :

```
$ helm uninstall release_name -n random
```

## 6. Running Integration and e2e Tests

The integration tests are run through Kubernetes itself. We use kuttl as the testing framework. The operator and the webhook must be running **as a Kubernetes deployment**.

Ensure that your Kubernetes cluster has a default storageClass. Most of the e2e tests do not specify any storageClass and use the default. For details about setting your storageClass, refer to the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/).

1. Push the operator and webhook with the following command:
   ```
   $ make docker-build-operator docker-push-operator
   ```
2. Verify that your operator Dockerfile is up-to-date before starting the tests.
   ```
   $ make create-helm-charts
   ```
3. Start the test with the following make target:
   ```
   $ make run-int-tests
   ```

You can also call `kubectl kuttl` from the command line if you want more control, such as running a single test or preventing cleanup when the test ends. For example, you can run a single e2e test and persist namespace with this command:

```
$ kubectl kuttl test --test <name-of-your-test> --skip-delete
```

For a complete list of flags that you can use, see the [kuttl docs](https://kuttl.dev/docs/cli.html#flags).

## 7. Running Soak Tests

The soak test will test the operator over a long interval. It splits the test into multiple iterations. Each iteration generates a random workload that is comprised of pod kills and scaling. At the end of each iteration, the test waits for everything to come up. If the test is successful, it proceeds to another iteration. It repeats this process for a set number of iterations or indefinitely.

The tests in an iteration are run through kuttl.  The random test generation is done by the kuttl-step-gen tool.

You can run this test with the following make target:

```
$ make run-soak-tests
```

## Help

```
$ make help
```

# Problem Determination

The following sections provide troubleshooting tips for your deployment.

## Kubernetes Events

The operator generates Kubernetes events for some key scenarios. This can be a useful tool when trying to understand what the operator is doing. Use the following command to view the events:

```
$ kubectl describe vdb mydb

...<snip>...
Events:
  Type    Reason                   Age    From                Message
  ----    ------                   ----   ----                -------
  Normal  Installing               2m10s  verticadb-operator  Calling update_vertica to add the following pods as new hosts: mydb-sc1-0
  Normal  InstallSucceeded         2m6s   verticadb-operator  Successfully called update_vertica to add new hosts and it took 3.5882135s
  Normal  CreateDBStart            2m5s   verticadb-operator  Calling 'admintools -t create_db'
  Normal  CreateDBSucceeded        92s    verticadb-operator  Successfully created database with subcluster 'sc1'. It took 32.5709857s
  Normal  ClusterRestartStarted    36s    verticadb-operator  Calling 'admintools -t start_db' to restart the cluster
  Normal  ClusterRestartSucceeded  28s    verticadb-operator  Successfully called 'admintools -t start_db' and it took 8.8401312s
```

## Retrieving Logs with vertica.log

You might need to inspect the contents of the vertica.log to diagnose a problem with the Vertica server.  There are a few ways this can be done:

- Drop into the container and navigate to the directory where is is stored. The exact location depends on your CR.  You can refer to the [Vertica documentation](https://www.vertica.com/docs/11.0.x/HTML/Content/Authoring/AdministratorsGuide/Monitoring/Vertica/MonitoringLogFiles.htm) to find the location.  

- Deploy a sidecar to captures the vertica.log and prints it to stdout.  If this sidecar is enabled you can use `kubectl logs` to inspect it.  This sidecar can be used by adding the following into your CR:

  ```
    sidecars:
      - name: vlogger
        image: <imageName>
  ```

  The image <imageName> is a container that you build yourself.  Vertica provides an example container in `docker-vlogger`. After it is running, inspect the logs by using this sidecar:

  ```
  $ kubectl logs vertica-sc1-0 -c vlogger
  ```

## Memory Profiling

The memory profiler lets you view where the big allocations are occurring and to help detect any memory leaks. The toolset is [Google's pprof](https://golang.org/pkg/net/http/pprof/).

By default, the memory profiler is disabled. To enable it, add a parameter when you start the operator.  The following steps enable the memory profiler for a deployed operator.

1. Use `kubectl edit` to open the running deployment for editing:

   ```
   $ kubectl edit deployment operator-controller-manager
   ```

2. Locate where the arguments are passed to the manager, and add `--enable-profiler`:

   ```
         ...
         - args:
           - --health-probe-bind-address=:8081
           - --metrics-bind-address=127.0.0.1:8080
           - --leader-elect
           - --enable-profiler
           command:
           - /manager
         ...
   ```

2. Wait until the operator is redeployed.
3. Port forward 6060 to access the webUI for the profiler. The name of the pod differs for each deployment, so be sure to find the one specific to your cluster:

   ```
   $ kubectl port-forward --address 10.20.30.40 pod/operator-controller-manager-5dd5b54df4-2krcr 6060:6060
   ```

4. Use a web browser or the standalone tool to connect to `http://localhost:6060/debug/pprof`.
   If you use a web browser, replace `localhost` with the host that you used in the previous `kubectl port-forward` command.
   Invoke the standalone tool with the following command:

   ```
   $ go tool pprof http://localhost:6060/debug/pprof
   ```
